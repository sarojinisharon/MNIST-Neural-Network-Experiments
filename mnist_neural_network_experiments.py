# -*- coding: utf-8 -*-
"""MNIST Neural Network Experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zyZRf7McbSmn1HaI-BEVw6y3qRq1rA5p

**Objective: **
This code defines a Sequential neural network model using TensorFlow and Keras. It consists of layers for flattening input data, two hidden layers with different numbers of units and ReLU activation functions, a dropout layer for regularization, and an output layer for multi-class classification using softmax activation.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Objective :
Training my own model on MNIST dataset and trying different architectures and hyperparameters, and testing it on a custom dataset.


"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.preprocessing import image
import numpy as np
import os
from PIL import Image
import matplotlib.pyplot as plt

# Loading the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

y_train

"""**Constructing my own model **

"""

model = tf.keras.models.Sequential([

    # Adding a convolutional layer with 32 filters, a 3x3 kernel, and 'relu' activation
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='layers_conv2d'),

    # Flattening the input (28x28 images) into a 1D array (784 elements)
    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),

    # Adding a hidden layer with 512 units and 'relu' activation for capturing higher-frequency features
    tf.keras.layers.Dense(512, activation='relu', name='layers_dense_512'),

    # Adding a dropout layer to prevent overfitting
    #tf.keras.layers.Dropout(0.2, name='layers_dropout'),

    # Adding another hidden layer with 256 units and 'relu' activation
    tf.keras.layers.Dense(256, activation='relu', name='layers_dense_256'),

    # Adding the output layer with 10 units (one for each digit) and 'softmax' activation
    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')
])

#Compiling the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Training the model for at least 10 epochs
history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

# Ploting the training and validation accuracy curves
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

#Training the model for at least 100 epochs to see overfitting
history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))

# Ploting the training and validation accuracy curves
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

# Configuration 1: Removed a hidden layer
model1 = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(256, activation='relu'),
])

#Compiling the model
model1.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history1 = model1.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

# Configuration 2: Increased Dropout
model2 = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),  # Increased dropout rate
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model2.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])
history2 = model2.fit(x_train, y_train, epochs=10, validation_data=(x_train, y_train))

# Configuration 3: Added Batch Normalization
model3 = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.BatchNormalization(),  # Added Batch Normalization
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model3.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

history3 = model3.fit(x_train, y_train, epochs=10, validation_data=(x_train, y_train))

# Ploting the training and validation accuracy curves
plt.figure(figsize=(10, 6))
plt.plot(history1.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(history2.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history2.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(history3.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history3.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

# Loading custom images from Google Drive
custom_images = []
custom_labels = []
base_folder_path = '/content/drive/MyDrive/TME_6015'
for folder_name in os.listdir(base_folder_path):
    folder_path = os.path.join(base_folder_path, folder_name)
    print(folder_path)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
          print('filename', filename)
          if filename.endswith('.png') or filename.endswith('.PNG') or filename.endswith('.jpg'):
              img_path = os.path.join(folder_path, filename)
              img = Image.open(img_path).convert('L')  # convert image to grayscale
              img = img.resize((28, 28))  # resize image to match MNIST data
              #threshold = 100
              #img = img.point(lambda p: p > threshold and 255)
              img_array = np.array(img)
              custom_images.append(img_array)
              custom_labels.append(int(folder_name))  # use folder name as label

# Converting the list of custom images and labels to numpy arrays
custom_images = np.array(custom_images)
custom_labels = np.array(custom_labels)

# Preprocessing the custom images
custom_images = custom_images / 255.0

custom_labels

display_image = custom_images[0] * 255

# Converting the image data to uint8
display_image = display_image.astype(np.uint8)

# Creating a PIL image
img = Image.fromarray(display_image)

# Display the image
img

x_test_new = (custom_images)
y_test_new = (custom_labels)

display_image = x_train[-1] * 255

# Convert the image data to uint8
display_image = display_image.astype(np.uint8)

# Create a PIL image
img = Image.fromarray(display_image)
img

model_custom = tf.keras.models.Sequential([
    # Flatten the input (28x28 images) into a 1D array (784 elements)
    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),

    # Add a hidden layer with 512 units and 'relu' activation for capturing higher-frequency features
    tf.keras.layers.Dense(512, activation='relu', name='layers_dense_512'),

    # Add a dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.2, name='layers_dropout'),

    # Add another hidden layer with 256 units and 'relu' activation
    tf.keras.layers.Dense(256, activation='relu', name='layers_dense_256'),

    # Add the output layer with 10 units (one for each digit) and 'softmax' activation
    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')
])

#Compile the model
model_custom.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model for at least 10 epochs
history_custom = model_custom.fit(x_train, y_train, epochs=10, validation_data=(x_test_new, y_test_new))

# Plot the training and validation accuracy curves
plt.figure(figsize=(10, 6))
plt.plot(history3.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history3.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Curves')
plt.legend()
plt.grid(True)
plt.show()

"""
I initiated the experimentation by training the model on the MNIST dataset for 10 epochs.  The following observations were made:

- **10 Epochs Accuracy:** Initially, after just 10 epochs of training, the model achieved an  accuracy of 99% on the MNIST dataset.

- **Checking Overfitting:** After 100 epochs, it was observed that a slight decline in accuracy, with the model achieving approximately 96%. This decline suggests that the model started to overfit the training data, as the training accuracy continued to improve while the validation accuracy plateued.

Then tried three different model configurations, each with distinct architecture or training parameter changes. These configurations were trained for 10 epochs, and the results were as follows:

**Model 1: Reduced Hidden Layer**
- In this configuration, tried removing one hidden layer from the original model architecture.
- Result: The accuracy after 10 epochs plummeted to a mere 10%. This significant drop in accuracy indicated that reducing the complexity of the model had a detrimental effect on its performance.

**Model 2: Increased Dropout**
- In this configuration, tried increasing the dropout rate to 0.5, aiming to introduce stronger regularization.
- Result: The accuracy after 10 epochs stabilized at 93%. This demonstrated that higher dropout effectively reduced overfitting but also led to a decrease in training accuracy.

**Model 3: Batch Normalization**
- In this configuration, tried incorporating batch normalization layers to enhance training stability and potentially boost accuracy.
- Result: The model achieved an impressive accuracy of 99% after 10 epochs. However, it was observed that the model was exhibiting signs of overfitting.

**Custom Dataset Testing**

While created a custom dataset and tested the model's performance on it. Surprisingly, the results closely mirrored our performance on the MNIST dataset:

- **Custom Dataset Accuracy:** The model achieved a commendable accuracy of 97% on the custom dataset. This accuracy aligns closely with our performance on the MNIST dataset, indicating that the model generalized well to the custom data.

**Conclusion:**

- It was observed that prolonged training (100 epochs) led to a minor decline in accuracy on the MNIST dataset, indicating the onset of overfitting.
- Experimenting with different model configurations revealed that architectural changes and hyperparameter adjustments could have substantial impacts on model performance. In particular, adding batch normalization significantly improved accuracy but also increased overfitting.
- The model displayed robust generalization capabilities, achieving a consistent 97% accuracy on both the MNIST dataset and our custom dataset. This suggests that the model learned meaningful features and was not overly sensitive to dataset variations.

Overall, these experiments underscore the importance of model configuration and the need to strike a balance between complexity and regularization to optimize model performance while mitigating overfitting."""